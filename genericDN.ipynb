{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuhubREvrLuN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "\n",
        "class LSTMPackedModelWithScalars(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size_seq, output_size_scalars):\n",
        "        super(LSTMPackedModelWithScalars, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "        # Fully connected layer for sequence output\n",
        "        self.fc_sequence = nn.Linear(hidden_size, output_size_seq)\n",
        "\n",
        "        # Fully connected layer for scalar output\n",
        "        self.fc_scalar = nn.Linear(hidden_size, output_size_scalar)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        # Pack the padded sequence\n",
        "        packed_input = rnn_utils.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        # LSTM forward pass\n",
        "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
        "\n",
        "        # Unpack the sequence\n",
        "        output, _ = rnn_utils.pad_packed_sequence(packed_output, batch_first=True)\n",
        "        #output = packed_output.data\n",
        "        # Sequence prediction (for each time step)\n",
        "        seq_output = self.fc_sequence(output)\n",
        "\n",
        "        # Scalar prediction (based on the last hidden state of LSTM)\n",
        "        # `hn` contains the hidden state for the last time step\n",
        "        scalar_output = self.fc_scalar(hn[-1])  # Take the hidden state of the last LSTM layer\n",
        "\n",
        "        return seq_output, scalar_output\n",
        "\n",
        "# Example usage:\n",
        "batch_size = 3\n",
        "seq_lengths = torch.tensor([4, 2, 1])  # Variable lengths of the sequences\n",
        "input_size = 2\n",
        "hidden_size = 16\n",
        "num_layers = 2\n",
        "output_size_seq = 1  # Sequence output size\n",
        "output_size_scalar = 2  # Scalars output size\n",
        "\n",
        "model = LSTMPackedModelWithScalars(input_size, hidden_size, num_layers, output_size_seq, output_size_scalar)"
      ]
    }
  ]
}