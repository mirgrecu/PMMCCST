{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-22 22\n"
     ]
    }
   ],
   "source": [
    "import netCDF4 as nc\n",
    "import glob\n",
    "import numpy as np\n",
    "fs=glob.glob(\"output/subset_*.nc\")\n",
    "fs=sorted(fs)\n",
    "y_lastbin=[]\n",
    "\n",
    "x_data=[]\n",
    "y_data=[]\n",
    "p_type_data=[]\n",
    "n_seq_data=[]\n",
    "#print(fs)\n",
    "y_nonz=[[]for k in range(44+18)]\n",
    "x_nonz=[[]for k in range(44+18)]\n",
    "xL=[]\n",
    "yL=[]\n",
    "diffL=[]\n",
    "for f in fs:\n",
    "    with nc.Dataset(f) as fh:\n",
    "        bin_nodes=fh.variables['bin_nodes'][:]\n",
    "        diffL.extend(bin_nodes[:,-1]-(bin_nodes[:,2]-2))\n",
    "\n",
    "print(np.array(diffL).min(),np.array(diffL).max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "645\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#h=plt.hist(diffL,bins=20)\n",
    "print(len(fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 322/322 [00:54<00:00,  5.88it/s]\n"
     ]
    }
   ],
   "source": [
    "n_files=len(fs)\n",
    "x_data=[]\n",
    "y_data=[]\n",
    "x_pia_data=[]\n",
    "y_pia_data=[]\n",
    "p_type_data=[]\n",
    "n_seq_data=[]\n",
    "bin_data=[]\n",
    "z_meas2L=[]\n",
    "bin_nodesL=[]\n",
    "pia_dataL=[]\n",
    "import tqdm\n",
    "for f in tqdm.tqdm(fs[:n_files//2]):\n",
    "    with nc.Dataset(f) as fh:\n",
    "        \n",
    "        vars=fh.variables.keys()\n",
    "        z_meas=fh.variables['z_meas'][:]\n",
    "        p_type=fh.variables['p_type'][:]\n",
    "        bin_nodes=fh.variables['bin_nodes'][:]\n",
    "        sfc_bin=fh.variables['sfc_bin'][:]\n",
    "        p_rate_cmb=fh.variables['p_rate_cmb'][:]\n",
    "        dm=fh.variables['dm'][:]\n",
    "        bin_zero_deg=fh.variables['bin_zero_deg'][:]\n",
    "        bin_storm_top=fh.variables['bin_storm_top'][:]\n",
    "        surface_type=fh.variables['surface_type'][:]\n",
    "        pia=fh.variables['pia'][:]\n",
    "        z_meas2=np.log10(0.5*10**(z_meas[:,::2]/10)+0.5*10**(z_meas[:,1::2]/10)+1e-9)*10\n",
    "        z_meas2[z_meas2<0]=0\n",
    "        for i,z_meas_1d in enumerate(z_meas2):\n",
    "            if surface_type[i]==0:\n",
    "                continue\n",
    "            if p_type[i]==3:\n",
    "                continue\n",
    "            itop=bin_storm_top[i]//2\n",
    "            x_1d=[]\n",
    "            y_1d=[]\n",
    "            i_valid=0\n",
    "            bin_1d=[]\n",
    "            for k1 in range(bin_nodes[i,0],bin_nodes[i,-1]): \n",
    "                x_1d.append([z_meas_1d[k1],k1-bin_nodes[i,2]])\n",
    "                y_1d.append([np.log10(1+p_rate_cmb[i,k1]/0.1),dm[i,k1]])\n",
    "                bin_1d.append(k1-bin_nodes[i,2])\n",
    "                i_valid+=1\n",
    "            \n",
    "            if(i_valid>0):\n",
    "                n_seq_data.append(i_valid)\n",
    "                bin_data.extend(bin_1d)\n",
    "                x_data.append(x_1d)\n",
    "                k1=bin_nodes[i,-1]\n",
    "                x_pia_data.append([sfc_bin[i,0]-bin_nodes[i,-1]])\n",
    "                y_pia_data.append([pia[i,0],np.log10(1+p_rate_cmb[i,k1]/0.1),dm[i,k1]])\n",
    "                y_data.append(y_1d)\n",
    "                p_type_data.append(p_type[i])\n",
    "                z_meas2L.append(z_meas_1d.copy())\n",
    "                bin_nodesL.append(bin_nodes[i,:])\n",
    "    #break\n",
    "\n",
    "#vars='z_meas', 'p_type', 'bin_nodes', 'sfc_bin', 'p_rate_cmb', 'dm', 'bin_zero_deg', 'bin_storm_top'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254344\n",
      "70\n"
     ]
    }
   ],
   "source": [
    "print(len(x_data))\n",
    "print(np.max(n_seq_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254344\n",
      "(254344, 1)\n"
     ]
    }
   ],
   "source": [
    "#plt.plot(x_data[25000][:,0])\n",
    "n_all=len(x_data)\n",
    "print(n_all)\n",
    "x_data_unpacked=np.zeros((n_all,72,2),np.float32)\n",
    "y_data_unpacked=np.zeros((n_all,72,2),np.float32)\n",
    "z_meas2L=np.array(z_meas2L)\n",
    "bin_nodesL=np.array(bin_nodesL)\n",
    "x_pia_data=np.array(x_pia_data)\n",
    "y_pia_data=np.array(y_pia_data)\n",
    "for i in range(n_all):\n",
    "    nz=len(x_data[i])\n",
    "    x_data_unpacked[i,:nz,:]=np.array(x_data[i])\n",
    "    x_data_unpacked[i,:nz,:1]=(x_data_unpacked[i,:nz,:1]-12)/8\n",
    "    x_data_unpacked[i,:nz,1:2]/=8.0\n",
    "    y_data_unpacked[i,:nz,:]=np.array(y_data[i])\n",
    "\n",
    "print(x_pia_data.shape) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(663803, 88) 663803\n",
      "[ 0.17286491 -0.875     ] [ 0.17286491 -0.875     ]\n",
      "[ 0.37375569 -0.75      ] [ 0.3737557 -0.75     ]\n",
      "[ 0.06635046 -0.625     ] [ 0.06635046 -0.625     ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "ix=np.random.randint(0,n_all)\n",
    "#ix=3000\n",
    "print(z_meas2L.shape,n_all)\n",
    "\n",
    "for k in range(bin_nodesL[ix,0],bin_nodesL[ix,-1]):\n",
    "    x1=[z_meas2L[ix][k],k-bin_nodesL[ix,2]]\n",
    "    x1[0]=(x1[0]-12)/8\n",
    "    x1[1]=x1[1]/8\n",
    "    xL.append(x1)\n",
    "    print(np.array(x1),x_data_unpacked[ix,k-bin_nodesL[ix,0],:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "class LSTMPackedModel(nn.Module):\n",
    "    def __init__(self, input_size, second_input_size, hidden_size, num_layers, output_size, second_output_size, n_comp):\n",
    "        super(LSTMPackedModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)  # Final output layer\n",
    "        self.sec_out_fc1=nn.Linear(second_input_size+hidden_size,hidden_size)\n",
    "        self.sec_out_fc2=nn.Linear(hidden_size,(n_comp+2)*second_output_size)\n",
    "    def forward(self, x, lengths, x2):\n",
    "        # Pack the padded sequence\n",
    "        packed_input = rnn_utils.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # LSTM forward pass\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "        #print(packed_output.data.shape)\n",
    "\n",
    "        # Unpack the sequence\n",
    "        output, _ = rnn_utils.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        #print(output.shape)\n",
    "        n_batch = len(lengths)\n",
    "        batch_indices = torch.arange(n_batch)\n",
    "        last_output = output[batch_indices,lengths-1,:]  \n",
    "        last_output=torch.cat([last_output,x2],dim=1)\n",
    "        # Apply the output layer to the LSTM output\n",
    "        output_1 = self.fc(output)\n",
    "        hid1=self.sec_out_fc1(last_output)\n",
    "        # apply relu\n",
    "        hid1=torch.relu(hid1)\n",
    "        output_2 = self.sec_out_fc2(hid1)\n",
    "\n",
    "        return output_1, output_2\n",
    "\n",
    "\n",
    "# define an ensemble of LSTMPackedModel models \n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Assume input is a padded batch with variable sequence lengths\n",
    "i_test=0\n",
    "if i_test==1:\n",
    "    batch_size = 4\n",
    "    input_size = 2\n",
    "    second_input_size = 1\n",
    "    second_output_size = 3\n",
    "    hidden_size = 20\n",
    "    num_layers = 2\n",
    "    output_size = 2\n",
    "    n_comp=2\n",
    "    seq_lengths = torch.tensor([41, 21, 12, 33]) \n",
    "    padded_input = torch.randn(batch_size, seq_lengths.max(), input_size)\n",
    "    second_input = torch.randn(batch_size,second_input_size) # Variable lengths of the sequences\n",
    "    input_size = 2\n",
    "\n",
    "\n",
    "    model = LSTMPackedModel(input_size, second_input_size, hidden_size, num_layers, output_size, second_output_size, n_comp)\n",
    "\n",
    "    padded_input = torch.tensor(x_data_unpacked[:10,:,:])\n",
    "    seq_lengths = torch.tensor(n_seq_data[:10])\n",
    "    second_input = torch.tensor(x_pia_data[:10])\n",
    "\n",
    "\n",
    "#    Forward pass   \n",
    "    output_1, output_2 = model(padded_input, seq_lengths, second_input)\n",
    "\n",
    "    print(output_1.shape)\n",
    "    print(output_2.shape)\n",
    "\n",
    "from torch.distributions.lowrank_multivariate_normal import LowRankMultivariateNormal\n",
    "def NLLLoss(output, target, sec_out_size):\n",
    "    nb, n2 = output.shape\n",
    "    n1=sec_out_size\n",
    "    ncomp=n2//sec_out_size-2\n",
    "    mean = output[:,0:n1]\n",
    "    cov_diag = torch.exp(output[:,n1:2*n1])+0.01\n",
    "    factors = output[:,2*n1:].reshape(nb, n1, ncomp)\n",
    "    dist = LowRankMultivariateNormal(mean, factors, cov_diag)\n",
    "    return -dist.log_prob(target).mean()\n",
    "\n",
    "if i_test==1:\n",
    "    target = torch.randn(10, 2)\n",
    "    loss = NLLLoss(output_2, target, second_output_size)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(254344, 72, 2)\n",
      "(254344, 72, 2)\n",
      "torch.Size([254344, 72, 2])\n"
     ]
    }
   ],
   "source": [
    "x_data_n=np.array(x_data_unpacked)\n",
    "y_data_n=np.array(y_data_unpacked)\n",
    "print(x_data_n.shape)\n",
    "print(y_data_n.shape)\n",
    "x_data_n=x_data_n\n",
    "y_data_n=y_data_n\n",
    "n_ens=5\n",
    "n_all=len(x_data_n)\n",
    "n_all_sub=n_all//n_ens\n",
    "\n",
    "x_data_t=torch.tensor(x_data_n/25.0,dtype=torch.float32)\n",
    "y_data_t=torch.tensor(y_data_n,dtype=torch.float32)\n",
    "x_pia_data_t=torch.tensor(x_pia_data,dtype=torch.float32)\n",
    "y_pia_data_t=torch.tensor(y_pia_data,dtype=torch.float32)\n",
    "print(x_data_t.shape)\n",
    "n_seq_data_t=torch.tensor(n_seq_data,dtype=torch.int32)\n",
    "p_type_data=np.array(p_type_data)\n",
    "a=np.nonzero(p_type_data>0)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34551,)\n",
      "34551\n",
      "11517\n"
     ]
    }
   ],
   "source": [
    "print(p_type_data[a].shape)\n",
    "a=np.nonzero(p_type_data==2)[0]\n",
    "ns1=1\n",
    "n_ens=3\n",
    "#print(n_seq_data)\n",
    "x_data_strat_n=torch.tensor(x_data_n[a][::ns1],dtype=torch.float32)\n",
    "y_data_strat_n=torch.tensor(y_data_n[a][::ns1],dtype=torch.float32)\n",
    "x_pia_data_strat=torch.tensor(np.array(x_pia_data)[a][::ns1],dtype=torch.float32)\n",
    "y_pia_data_strat=torch.tensor(np.array(y_pia_data)[a][::ns1],dtype=torch.float32)\n",
    "n_seq_data_strat=torch.tensor(np.array(n_seq_data)[a][::ns1],dtype=torch.int32)\n",
    "\n",
    "strat_dataL=[]\n",
    "strat_loaderL=[]\n",
    "n_all_str=x_data_strat_n.shape[0]\n",
    "print(n_all_str)\n",
    "n_all_sub=n_all_str//n_ens\n",
    "print(n_all_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 196.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([11517, 72, 2])\n",
      "1 torch.Size([11517, 72, 2])\n",
      "2 torch.Size([11517, 72, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in tqdm.tqdm(range(n_ens)):\n",
    "    strat_data=torch.utils.data.TensorDataset(x_data_strat_n[i*n_all_sub:(i+1)*n_all_sub,:,:],y_data_strat_n[i*n_all_sub:(i+1)*n_all_sub,:,:],n_seq_data_strat[i*n_all_sub:(i+1)*n_all_sub],x_pia_data_strat[i*n_all_sub:(i+1)*n_all_sub,:],y_pia_data_strat[i*n_all_sub:(i+1)*n_all_sub,:])\n",
    "    print(i,strat_data.tensors[0].shape)\n",
    "    strat_loader=torch.utils.data.DataLoader(strat_data,batch_size=64,shuffle=True)\n",
    "    strat_dataL.append(strat_data)\n",
    "    strat_loaderL.append(strat_loader)\n",
    "    #print(i,strat_data.tensors[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.898073322698474\n",
      "1 -2.339838701941901\n",
      "2 -2.7656547717957034\n",
      "3 -2.8127212063926788\n",
      "4 -2.876323032565415\n",
      "5 -2.9760439957181615\n",
      "6 -3.0484896474000482\n",
      "7 -3.089240730388297\n",
      "8 -3.1189644364847076\n",
      "9 -3.141674852805833\n",
      "10 -3.1761795971542597\n",
      "11 -3.175427285167906\n",
      "12 -3.213492615169121\n",
      "13 -3.235259349292351\n",
      "14 -3.2562927286554544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x_/d2_jzyq50052xh1_tk02bnmc0000gq/T/ipykernel_4150/1131222009.py:23: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  n_batch = len(lengths)\n",
      "[W218 10:41:17.947657000 shape_type_inference.cpp:1999] Warning: The shape inference of prim::PackPadded type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W218 10:41:17.950243000 shape_type_inference.cpp:1999] Warning: The shape inference of prim::PackPadded type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "/Users/mgrecu/miniforge3/lib/python3.12/site-packages/torch/onnx/symbolic_opset9.py:4279: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n",
      "[W218 10:41:17.105326000 shape_type_inference.cpp:1999] Warning: The shape inference of prim::PadPacked type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "/Users/mgrecu/miniforge3/lib/python3.12/site-packages/torch/onnx/symbolic_opset9.py:5385: UserWarning: Exporting aten::index operator of advanced indexing in opset 11 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
      "  warnings.warn(\n",
      " 33%|███▎      | 1/3 [08:36<17:12, 516.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7537125536137157\n",
      "1 -2.2442228705104856\n",
      "2 -2.680083980369899\n",
      "3 -2.8551936354695093\n",
      "4 -2.8549209251999854\n",
      "5 -2.9626000865466064\n",
      "6 -3.0339454938761063\n",
      "7 -3.0716992757179673\n",
      "8 -3.026126131291191\n",
      "9 -3.143761848948068\n",
      "10 -3.1321229713865453\n",
      "11 -3.184983081743121\n",
      "12 -3.199439244489703\n",
      "13 -3.2332118831160996\n",
      "14 -3.2327727860874598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W218 10:49:41.512705000 shape_type_inference.cpp:1999] Warning: The shape inference of prim::PackPadded type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W218 10:49:41.513880000 shape_type_inference.cpp:1999] Warning: The shape inference of prim::PackPadded type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W218 10:49:41.662170000 shape_type_inference.cpp:1999] Warning: The shape inference of prim::PadPacked type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      " 67%|██████▋   | 2/3 [16:59<08:28, 508.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6282210427853796\n",
      "1 -2.4004403583705427\n",
      "2 -2.655899199160437\n",
      "3 -2.833868743975957\n",
      "4 -2.9069929643430643\n",
      "5 -2.983242106851604\n",
      "6 -3.0062335507944224\n",
      "7 -3.0825338578265575\n",
      "8 -3.1063892021568287\n",
      "9 -3.132279234048393\n",
      "10 -3.1200731791555882\n",
      "11 -3.189896329699291\n",
      "12 -3.189248254357113\n",
      "13 -3.177522522976829\n",
      "14 -3.220627219809426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W218 10:58:03.911037000 shape_type_inference.cpp:1999] Warning: The shape inference of prim::PackPadded type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W218 10:58:03.913234000 shape_type_inference.cpp:1999] Warning: The shape inference of prim::PackPadded type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W218 10:58:03.060772000 shape_type_inference.cpp:1999] Warning: The shape inference of prim::PadPacked type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "100%|██████████| 3/3 [25:22<00:00, 507.37s/it]\n"
     ]
    }
   ],
   "source": [
    "n_epoch=15\n",
    "model_list=[]\n",
    "#\n",
    "lambda1=0.5\n",
    "n_ens3=3\n",
    "input_size, second_input_size, hidden_size, num_layers, output_size, second_output_size, n_comp=2,1,64,2,2,3,2\n",
    "for i in tqdm.tqdm(range(n_ens3)):\n",
    "    model_list.append(LSTMPackedModel(input_size, second_input_size, hidden_size, num_layers, output_size, second_output_size, n_comp))\n",
    "    model_strat=model_list[i]#\n",
    "    strat_criterion=nn.MSELoss(reduction='none')\n",
    "    strat_optimizer=torch.optim.Adam(model_strat.parameters(),lr=0.001)\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        running_loss=0\n",
    "        for iloader in range(n_ens3):\n",
    "            for x,y,n_seq, x_sec, y_sec in strat_loaderL[iloader]:\n",
    "                strat_optimizer.zero_grad()\n",
    "                y_pred1,y_pred2=model_strat(x,n_seq,x_sec)\n",
    "                n_seq1=y_pred1.shape[1]\n",
    "       \n",
    "                loss=strat_criterion(y_pred1,y[:,:n_seq1,:])\n",
    "                mask=torch.arange(n_seq1).expand(y.shape[0],n_seq1)<n_seq.unsqueeze(1)\n",
    "        \n",
    "                masked_loss=(loss*mask.unsqueeze(2)).sum()\n",
    "                masked_loss=masked_loss/mask.sum()\n",
    "                total_loss=masked_loss+lambda1*NLLLoss(y_pred2,y_sec,second_output_size)\n",
    "                total_loss.backward()\n",
    "                running_loss+=total_loss.item()\n",
    "                strat_optimizer.step()\n",
    "        print(epoch,running_loss/len(strat_loader))\n",
    "    torch.onnx.export(model_list[i],(x[:1][:,:,:],n_seq[:1],x_sec[:1]),'conv_ku_densi_net_land_feb16_2025_%2.2i.onnx'%i,export_params=True,opset_version=11,do_constant_folding=True,input_names=['input_1','n_seq','input_2'],output_names=['output1','output2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/var/folders/x_/d2_jzyq50052xh1_tk02bnmc0000gq/T/ipykernel_4150/1131222009.py:23: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  n_batch = len(lengths)\n",
      "[W216 16:48:33.228141000 shape_type_inference.cpp:1999] Warning: The shape inference of prim::PackPadded type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W216 16:48:33.229215000 shape_type_inference.cpp:1999] Warning: The shape inference of prim::PackPadded type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "/Users/mgrecu/miniforge3/lib/python3.12/site-packages/torch/onnx/symbolic_opset9.py:4279: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n",
      "[W216 16:48:33.377565000 shape_type_inference.cpp:1999] Warning: The shape inference of prim::PadPacked type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "/Users/mgrecu/miniforge3/lib/python3.12/site-packages/torch/onnx/symbolic_opset9.py:5385: UserWarning: Exporting aten::index operator of advanced indexing in opset 11 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
      "  warnings.warn(\n",
      " 33%|███▎      | 1/3 [00:00<00:00,  2.44it/s][W216 16:48:33.605977000 shape_type_inference.cpp:1999] Warning: The shape inference of prim::PackPadded type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W216 16:48:33.607267000 shape_type_inference.cpp:1999] Warning: The shape inference of prim::PackPadded type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W216 16:48:33.754814000 shape_type_inference.cpp:1999] Warning: The shape inference of prim::PadPacked type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      " 67%|██████▋   | 2/3 [00:00<00:00,  2.55it/s][W216 16:48:33.985191000 shape_type_inference.cpp:1999] Warning: The shape inference of prim::PackPadded type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W216 16:48:33.986335000 shape_type_inference.cpp:1999] Warning: The shape inference of prim::PackPadded type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W216 16:48:33.133122000 shape_type_inference.cpp:1999] Warning: The shape inference of prim::PadPacked type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.58it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm.tqdm(range(n_ens3)):\n",
    "    torch.onnx.export(model_list[i],(x[:1][:,:,:],n_seq[:1],x_sec[:1]),'conv_ku_densi_net_feb16_2025_%2.2i.onnx'%i,export_params=True,opset_version=11,do_constant_folding=True,input_names=['input_1','n_seq','input_2'],output_names=['output1','output2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1,y_pred2=model_list[0](x_data_strat_n,n_seq_data_strat,x_pia_data_strat)\n",
    "y_pred1_np=y_pred1.detach().numpy()\n",
    "y_pred2_np=y_pred2.detach().numpy()\n",
    "y_test_np=y_data_strat_n.detach().numpy()\n",
    "y_pia_test_np=y_pia_data_strat.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.94791359]\n",
      " [0.94791359 1.        ]]\n",
      "[[1.        0.9580755]\n",
      " [0.9580755 1.       ]]\n"
     ]
    }
   ],
   "source": [
    "print(np.corrcoef(y_pred2_np[:,1],y_pia_test_np[:,1]))\n",
    "print(np.corrcoef(y_pred2_np[:,0],y_pia_test_np[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.max(\n",
      "values=tensor([18.]),\n",
      "indices=tensor([11472]))\n"
     ]
    }
   ],
   "source": [
    "print(x_pia_data_strat.max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.03449178049541849\n",
      "1 0.03447961658284032\n",
      "2 0.03437462921881919\n",
      "3 0.034343278710004525\n",
      "4 0.03427321770467418\n",
      "5 0.03410242947836896\n",
      "6 0.03394827105069189\n",
      "7 0.03378847528615301\n",
      "8 0.033944862818778704\n",
      "9 0.03381727487563515\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(n_epoch2):\n",
    "    running_loss=0\n",
    "    for x,y,n_seq in strat_loader:\n",
    "        strat_ku_optimizer.zero_grad()\n",
    "        y_pred=model_ku_strat(x[:,:,a_feat],n_seq)\n",
    "        n_seq1=y_pred.shape[1]\n",
    "       \n",
    "        loss=strat_criterion(y_pred,y[:,:n_seq1,:2])\n",
    "        mask=torch.arange(n_seq1).expand(y.shape[0],n_seq1)<n_seq.unsqueeze(1)\n",
    "        \n",
    "        masked_loss=(loss*mask.unsqueeze(2)).sum()\n",
    "        masked_loss=masked_loss/mask.sum()\n",
    "    \n",
    "        masked_loss.backward()\n",
    "        running_loss+=masked_loss.item()\n",
    "        strat_ku_optimizer.step()\n",
    "    print(epoch,running_loss/len(strat_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x_/d2_jzyq50052xh1_tk02bnmc0000gq/T/ipykernel_4150/1131222009.py:23: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  n_batch = len(lengths)\n",
      "[W216 15:54:27.288979000 shape_type_inference.cpp:1999] Warning: The shape inference of prim::PackPadded type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W216 15:54:27.297769000 shape_type_inference.cpp:1999] Warning: The shape inference of prim::PackPadded type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "/Users/mgrecu/miniforge3/lib/python3.12/site-packages/torch/onnx/symbolic_opset9.py:4279: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n",
      "[W216 15:54:27.337131000 shape_type_inference.cpp:1999] Warning: The shape inference of prim::PadPacked type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "/Users/mgrecu/miniforge3/lib/python3.12/site-packages/torch/onnx/symbolic_opset9.py:5385: UserWarning: Exporting aten::index operator of advanced indexing in opset 11 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    torch.onnx.export(model_list[i],(x[:1][:,:,:],n_seq[:1],x_sec[:1]),'conv_ku_densi_net_feb16_2025_%2.2i.onnx'%i,export_params=True,opset_version=11,do_constant_folding=True,input_names=['input_1','n_seq','input_2'],output_names=['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(72, dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "print(n_seq_data_strat.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_strat.eval()\n",
    "output=model_strat(x_data_strat_n[::2,:,:],n_seq_data_strat[::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_np=output.detach().numpy()\n",
    "import pickle\n",
    "pickle_out={\"output\":output_np,\"y_data\":y_data_strat_n[::2,:,:].detach().numpy(),\"x_data\":x_data_strat_n[::2,:,:].detach().numpy(),\"n_seq\":n_seq_data_strat[::2].detach().numpy(),\"z_meas\":z_meas2_strat[::2],\"bin_nodes\":bin_nodesL_strat[::2]}\n",
    "pickle.dump(pickle_out,open(\"output_strat_jan28_2025.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W129 11:23:13.674287000 shape_type_inference.cpp:1999] Warning: The shape inference of prim::PackPadded type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W129 11:23:13.675937000 shape_type_inference.cpp:1999] Warning: The shape inference of prim::PackPadded type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "/Users/mgrecu/miniforge3/lib/python3.12/site-packages/torch/onnx/symbolic_opset9.py:4279: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n",
      "[W129 11:23:13.885182000 shape_type_inference.cpp:1999] Warning: The shape inference of prim::PadPacked type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(model_ku_strat,(x[:1][:,:,a_feat],n_seq[:1]),'conv_and_strat_model_ku_jan28_2025.onnx',export_params=True,opset_version=11,do_constant_folding=True,input_names=['input','n_seq'],output_names=['output'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
