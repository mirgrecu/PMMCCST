{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-22 22\n"
     ]
    }
   ],
   "source": [
    "import netCDF4 as nc\n",
    "import glob\n",
    "import numpy as np\n",
    "fs=glob.glob(\"output/subset_*.nc\")\n",
    "fs=sorted(fs)\n",
    "y_lastbin=[]\n",
    "\n",
    "x_data=[]\n",
    "y_data=[]\n",
    "p_type_data=[]\n",
    "n_seq_data=[]\n",
    "#print(fs)\n",
    "y_nonz=[[]for k in range(44+18)]\n",
    "x_nonz=[[]for k in range(44+18)]\n",
    "xL=[]\n",
    "yL=[]\n",
    "diffL=[]\n",
    "for f in fs:\n",
    "    with nc.Dataset(f) as fh:\n",
    "        bin_nodes=fh.variables['bin_nodes'][:]\n",
    "        diffL.extend(bin_nodes[:,-1]-(bin_nodes[:,2]-2))\n",
    "\n",
    "print(np.array(diffL).min(),np.array(diffL).max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#h=plt.hist(diffL,bins=20)\n",
    "print(len(fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 178/178 [01:24<00:00,  2.10it/s]\n"
     ]
    }
   ],
   "source": [
    "n_files=len(fs)\n",
    "x_data=[]\n",
    "y_data=[]\n",
    "x_pia_data=[]\n",
    "y_pia_data=[]\n",
    "p_type_data=[]\n",
    "n_seq_data=[]\n",
    "bin_data=[]\n",
    "z_meas2L=[]\n",
    "bin_nodesL=[]\n",
    "pia_dataL=[]\n",
    "import tqdm\n",
    "for f in tqdm.tqdm(fs[:n_files//2]):\n",
    "    with nc.Dataset(f) as fh:\n",
    "        \n",
    "        vars=fh.variables.keys()\n",
    "        z_meas=fh.variables['z_meas'][:]\n",
    "        p_type=fh.variables['p_type'][:]\n",
    "        bin_nodes=fh.variables['bin_nodes'][:]\n",
    "        sfc_bin=fh.variables['sfc_bin'][:]\n",
    "        p_rate_cmb=fh.variables['p_rate_cmb'][:]\n",
    "        dm=fh.variables['dm'][:]\n",
    "        bin_zero_deg=fh.variables['bin_zero_deg'][:]\n",
    "        bin_storm_top=fh.variables['bin_storm_top'][:]\n",
    "        surface_type=fh.variables['surface_type'][:]\n",
    "        pia=fh.variables['pia'][:]\n",
    "        z_meas2=np.log10(0.5*10**(z_meas[:,::2]/10)+0.5*10**(z_meas[:,1::2]/10)+1e-9)*10\n",
    "        z_meas2[z_meas2<0]=0\n",
    "        for i,z_meas_1d in enumerate(z_meas2):\n",
    "            if surface_type[i]!=0:\n",
    "                continue\n",
    "            if p_type[i]==3:\n",
    "                continue\n",
    "            itop=bin_storm_top[i]//2\n",
    "            x_1d=[]\n",
    "            y_1d=[]\n",
    "            i_valid=0\n",
    "            bin_1d=[]\n",
    "            for k1 in range(bin_nodes[i,0],bin_nodes[i,-1]): \n",
    "                x_1d.append([z_meas_1d[k1],k1-bin_nodes[i,2]])\n",
    "                y_1d.append([np.log10(1+p_rate_cmb[i,k1]/0.1),dm[i,k1]])\n",
    "                bin_1d.append(k1-bin_nodes[i,2])\n",
    "                i_valid+=1\n",
    "            \n",
    "            if(i_valid>0):\n",
    "                n_seq_data.append(i_valid)\n",
    "                bin_data.extend(bin_1d)\n",
    "                x_data.append(x_1d)\n",
    "                k1=bin_nodes[i,-1]\n",
    "                x_pia_data.append([sfc_bin[i,0]-bin_nodes[i,-1]])\n",
    "                y_pia_data.append([pia[i,0],np.log10(1+p_rate_cmb[i,k1]/0.1),dm[i,k1]])\n",
    "                y_data.append(y_1d)\n",
    "                p_type_data.append(p_type[i])\n",
    "                z_meas2L.append(z_meas_1d.copy())\n",
    "                bin_nodesL.append(bin_nodes[i,:])\n",
    "    #break\n",
    "\n",
    "#vars='z_meas', 'p_type', 'bin_nodes', 'sfc_bin', 'p_rate_cmb', 'dm', 'bin_zero_deg', 'bin_storm_top'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "663803\n",
      "68\n"
     ]
    }
   ],
   "source": [
    "print(len(x_data))\n",
    "print(np.max(n_seq_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "663803\n",
      "(663803, 1)\n"
     ]
    }
   ],
   "source": [
    "#plt.plot(x_data[25000][:,0])\n",
    "n_all=len(x_data)\n",
    "print(n_all)\n",
    "x_data_unpacked=np.zeros((n_all,72,2),np.float32)\n",
    "y_data_unpacked=np.zeros((n_all,72,2),np.float32)\n",
    "z_meas2L=np.array(z_meas2L)\n",
    "bin_nodesL=np.array(bin_nodesL)\n",
    "x_pia_data=np.array(x_pia_data)\n",
    "y_pia_data=np.array(y_pia_data)\n",
    "for i in range(n_all):\n",
    "    nz=len(x_data[i])\n",
    "    x_data_unpacked[i,:nz,:]=np.array(x_data[i])\n",
    "    x_data_unpacked[i,:nz,:1]=(x_data_unpacked[i,:nz,:1]-12)/8\n",
    "    x_data_unpacked[i,:nz,1:2]/=8.0\n",
    "    y_data_unpacked[i,:nz,:]=np.array(y_data[i])\n",
    "\n",
    "print(x_pia_data.shape) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(663803, 88) 663803\n",
      "[ 0.17286491 -0.875     ] [ 0.17286491 -0.875     ]\n",
      "[ 0.37375569 -0.75      ] [ 0.3737557 -0.75     ]\n",
      "[ 0.06635046 -0.625     ] [ 0.06635046 -0.625     ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "ix=np.random.randint(0,n_all)\n",
    "#ix=3000\n",
    "print(z_meas2L.shape,n_all)\n",
    "\n",
    "for k in range(bin_nodesL[ix,0],bin_nodesL[ix,-1]):\n",
    "    x1=[z_meas2L[ix][k],k-bin_nodesL[ix,2]]\n",
    "    x1[0]=(x1[0]-12)/8\n",
    "    x1[1]=x1[1]/8\n",
    "    xL.append(x1)\n",
    "    print(np.array(x1),x_data_unpacked[ix,k-bin_nodesL[ix,0],:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "class LSTMPackedModel(nn.Module):\n",
    "    def __init__(self, input_size, second_input_size, hidden_size, num_layers, output_size, second_output_size, n_comp):\n",
    "        super(LSTMPackedModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)  # Final output layer\n",
    "        self.sec_out_fc1=nn.Linear(second_input_size+hidden_size,hidden_size)\n",
    "        self.sec_out_fc2=nn.Linear(hidden_size,(n_comp+2)*second_output_size)\n",
    "    def forward(self, x, lengths, x2):\n",
    "        # Pack the padded sequence\n",
    "        packed_input = rnn_utils.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # LSTM forward pass\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "        print(packed_output.data.shape)\n",
    "\n",
    "        # Unpack the sequence\n",
    "        output, _ = rnn_utils.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        print(output.shape)\n",
    "        n_batch = len(lengths)\n",
    "        batch_indices = torch.arange(n_batch)\n",
    "        last_output = output[batch_indices,lengths-1,:]  \n",
    "        last_output=torch.cat([last_output,x2],dim=1)\n",
    "        # Apply the output layer to the LSTM output\n",
    "        output_1 = self.fc(output)\n",
    "        hid1=self.sec_out_fc1(last_output)\n",
    "        # apply relu\n",
    "        hid1=torch.relu(hid1)\n",
    "        output_2 = self.sec_out_fc2(hid1)\n",
    "\n",
    "        return output_1, output_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21, 20])\n",
      "torch.Size([10, 3, 20])\n",
      "torch.Size([10, 3, 2])\n",
      "torch.Size([10, 12])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# define an ensemble of LSTMPackedModel models \n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Assume input is a padded batch with variable sequence lengths\n",
    "batch_size = 4\n",
    "input_size = 2\n",
    "second_input_size = 1\n",
    "second_output_size = 3\n",
    "hidden_size = 20\n",
    "num_layers = 2\n",
    "output_size = 2\n",
    "n_comp=2\n",
    "seq_lengths = torch.tensor([41, 21, 12, 33]) \n",
    "padded_input = torch.randn(batch_size, seq_lengths.max(), input_size)\n",
    "second_input = torch.randn(batch_size,second_input_size) # Variable lengths of the sequences\n",
    "input_size = 2\n",
    "\n",
    "\n",
    "model = LSTMPackedModel(input_size, second_input_size, hidden_size, num_layers, output_size, second_output_size, n_comp)\n",
    "\n",
    "padded_input = torch.tensor(x_data_unpacked[:10,:,:])\n",
    "seq_lengths = torch.tensor(n_seq_data[:10])\n",
    "second_input = torch.tensor(x_pia_data[:10])\n",
    "\n",
    "\n",
    "# Forward pass\n",
    "output_1, output_2 = model(padded_input, seq_lengths, second_input)\n",
    "\n",
    "print(output_1.shape)\n",
    "print(output_2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 2])\n",
      "torch.Size([10, 12])\n",
      "(10, 3)\n"
     ]
    }
   ],
   "source": [
    "print(output_1.shape)\n",
    "print(output_2.shape)\n",
    "print(y_pia_data[:10].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions.lowrank_multivariate_normal import LowRankMultivariateNormal\n",
    "def NLLLoss(output, target, sec_out_size):\n",
    "    nb, n2 = output.shape\n",
    "    n1=sec_out_size\n",
    "    ncomp=n2//sec_out_size-2\n",
    "    mean = output[:,0:n1]\n",
    "    cov_diag = torch.exp(output[:,n1:2*n1])+0.01\n",
    "    factors = output[:,2*n1:].reshape(nb, n1, ncomp)\n",
    "    dist = LowRankMultivariateNormal(mean, factors, cov_diag)\n",
    "    return -dist.log_prob(target).mean()\n",
    "\n",
    "target = torch.tensor(y_pia_data[:10],dtype=torch.float32)\n",
    "loss = NLLLoss(output_2, target, second_output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(663803, 72, 2)\n",
      "(663803, 72, 2)\n",
      "torch.Size([663803, 72, 2])\n"
     ]
    }
   ],
   "source": [
    "x_data_n=np.array(x_data_unpacked)\n",
    "y_data_n=np.array(y_data_unpacked)\n",
    "print(x_data_n.shape)\n",
    "print(y_data_n.shape)\n",
    "x_data_n=x_data_n\n",
    "y_data_n=y_data_n\n",
    "n_ens=5\n",
    "n_all=len(x_data_n)\n",
    "n_all_sub=n_all//n_ens\n",
    "\n",
    "x_data_t=torch.tensor(x_data_n,dtype=torch.float32)\n",
    "y_data_t=torch.tensor(y_data_n,dtype=torch.float32)\n",
    "x_pia_data_t=torch.tensor(x_pia_data,dtype=torch.float32)\n",
    "y_pia_data_t=torch.tensor(y_pia_data,dtype=torch.float32)\n",
    "print(x_data_t.shape)\n",
    "n_seq_data_t=torch.tensor(n_seq_data,dtype=torch.int32)\n",
    "p_type_data=np.array(p_type_data)\n",
    "a=np.nonzero(p_type_data>0)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(136953,)\n",
      "68477\n"
     ]
    }
   ],
   "source": [
    "print(p_type_data[a].shape)\n",
    "a=np.nonzero(p_type_data==2)[0]\n",
    "#print(n_seq_data)\n",
    "x_data_strat_n=torch.tensor(x_data_n[a][::2],dtype=torch.float32)\n",
    "y_data_strat_n=torch.tensor(y_data_n[a][::2],dtype=torch.float32)\n",
    "x_pia_data_strat=torch.tensor(np.array(x_pia_data)[a][::2],dtype=torch.float32)\n",
    "y_pia_data_strat=torch.tensor(np.array(y_pia_data)[a][::2],dtype=torch.float32)\n",
    "n_seq_data_strat=torch.tensor(np.array(n_seq_data)[a][::2],dtype=torch.int32)\n",
    "\n",
    "strat_dataL=[]\n",
    "strat_loaderL=[]\n",
    "n_all_str=x_data_strat_n.shape[0]\n",
    "print(n_all_str)\n",
    "n_all_sub=n_all_str//n_ens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 2246.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([13695, 72, 2])\n",
      "0 torch.Size([13695, 72, 2])\n",
      "1 torch.Size([13695, 72, 2])\n",
      "1 torch.Size([13695, 72, 2])\n",
      "2 torch.Size([13695, 72, 2])\n",
      "2 torch.Size([13695, 72, 2])\n",
      "3 torch.Size([13695, 72, 2])\n",
      "3 torch.Size([13695, 72, 2])\n",
      "4 torch.Size([13695, 72, 2])\n",
      "4 torch.Size([13695, 72, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in tqdm.tqdm(range(n_ens)):\n",
    "    strat_data=torch.utils.data.TensorDataset(x_data_strat_n[i*n_all_sub:(i+1)*n_all_sub,:,:],y_data_strat_n[i*n_all_sub:(i+1)*n_all_sub,:,:],n_seq_data_strat[i*n_all_sub:(i+1)*n_all_sub],x_pia_data_strat[i*n_all_sub:(i+1)*n_all_sub,:],y_pia_data_strat[i*n_all_sub:(i+1)*n_all_sub,:])\n",
    "    print(i,strat_data.tensors[0].shape)\n",
    "    strat_loader=torch.utils.data.DataLoader(strat_data,batch_size=64,shuffle=True)\n",
    "    strat_dataL.append(strat_data)\n",
    "    strat_loaderL.append(strat_loader)\n",
    "    #print(i,strat_data.tensors[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.06934201862895861\n",
      "1 0.02420530130621046\n",
      "2 0.022658148509799504\n",
      "3 0.021992281836719484\n",
      "4 0.021407379979791585\n",
      "5 0.02106362636623089\n",
      "6 0.020827438218111637\n",
      "7 0.02064113713131519\n",
      "8 0.020419841608963908\n",
      "9 0.020350829797098412\n",
      "10 0.020209105785033897\n",
      "11 0.020079684624943184\n",
      "12 0.020033100412547356\n",
      "13 0.019952269347413676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [15:31<1:02:04, 931.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 0.019891637776163408\n",
      "0 0.06796584181574872\n",
      "1 0.02512463169041439\n",
      "2 0.02286907410962158\n",
      "3 0.022096279269317166\n",
      "4 0.021575996471074176\n",
      "5 0.021125539940112504\n",
      "6 0.020791261498379753\n",
      "7 0.020570674612827133\n",
      "8 0.020421929088479375\n",
      "9 0.020307379881705855\n",
      "10 0.02021378282806836\n",
      "11 0.020082360033120493\n",
      "12 0.020005288698303047\n",
      "13 0.019943869152484694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [31:19<47:03, 941.29s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 0.019867945359874285\n",
      "0 0.07384799766296055\n",
      "1 0.026351577714376618\n",
      "2 0.024380220949387875\n",
      "3 0.023723485785740194\n",
      "4 0.023149053770612226\n",
      "5 0.022868641231616492\n",
      "6 0.02247813692301861\n",
      "7 0.022140650155051844\n",
      "8 0.022049030769994715\n",
      "9 0.021710886362416203\n",
      "10 0.021680279221618547\n",
      "11 0.021610785748634953\n",
      "12 0.02147136582643725\n",
      "13 0.021443680006632347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [46:45<31:08, 934.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 0.021358884300570934\n",
      "0 0.06954631688713561\n",
      "1 0.025609556872950634\n",
      "2 0.024016199359903113\n",
      "3 0.02324647347049904\n",
      "4 0.02259410867554834\n",
      "5 0.02222356697457144\n",
      "6 0.021936603450012627\n",
      "7 0.021678154290566454\n",
      "8 0.021577572362002684\n",
      "9 0.02137623948874534\n",
      "10 0.021355465260421626\n",
      "11 0.021240044703881723\n",
      "12 0.02113234385906253\n",
      "13 0.021025945502333342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [1:01:42<15:19, 919.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 0.020987325462920127\n",
      "0 0.06751989782205783\n",
      "1 0.027066640510020078\n",
      "2 0.025383431928639767\n",
      "3 0.024577124524512328\n",
      "4 0.024035189689311665\n",
      "5 0.02353460068407003\n",
      "6 0.023254785277822522\n",
      "7 0.022990844793821453\n",
      "8 0.022773481126205298\n",
      "9 0.02265720828145277\n",
      "10 0.02255013150133891\n",
      "11 0.022430843003530754\n",
      "12 0.022356770417536607\n",
      "13 0.02227863672451349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [1:17:24<00:00, 928.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 0.02221139175380813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_epoch=15\n",
    "model_list=[]\n",
    "#\n",
    "for i in tqdm.tqdm(range(n_ens)):\n",
    "    model_list.append(LSTMPackedModel(2,64,3,2))\n",
    "    model_strat=model_list[i]#\n",
    "    strat_criterion=nn.MSELoss(reduction='none')\n",
    "    strat_optimizer=torch.optim.Adam(model_strat.parameters(),lr=0.001)\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        running_loss=0\n",
    "        for x,y,n_seq in strat_loaderL[i]:\n",
    "            strat_optimizer.zero_grad()\n",
    "            y_pred=model_strat(x,n_seq)\n",
    "            n_seq1=y_pred.shape[1]\n",
    "       \n",
    "            loss=strat_criterion(y_pred,y[:,:n_seq1,:])\n",
    "            mask=torch.arange(n_seq1).expand(y.shape[0],n_seq1)<n_seq.unsqueeze(1)\n",
    "        \n",
    "            masked_loss=(loss*mask.unsqueeze(2)).sum()\n",
    "            masked_loss=masked_loss/mask.sum()\n",
    "    \n",
    "            masked_loss.backward()\n",
    "            running_loss+=masked_loss.item()\n",
    "            strat_optimizer.step()\n",
    "        print(epoch,running_loss/len(strat_loader))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.03449178049541849\n",
      "1 0.03447961658284032\n",
      "2 0.03437462921881919\n",
      "3 0.034343278710004525\n",
      "4 0.03427321770467418\n",
      "5 0.03410242947836896\n",
      "6 0.03394827105069189\n",
      "7 0.03378847528615301\n",
      "8 0.033944862818778704\n",
      "9 0.03381727487563515\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(n_epoch2):\n",
    "    running_loss=0\n",
    "    for x,y,n_seq in strat_loader:\n",
    "        strat_ku_optimizer.zero_grad()\n",
    "        y_pred=model_ku_strat(x[:,:,a_feat],n_seq)\n",
    "        n_seq1=y_pred.shape[1]\n",
    "       \n",
    "        loss=strat_criterion(y_pred,y[:,:n_seq1,:2])\n",
    "        mask=torch.arange(n_seq1).expand(y.shape[0],n_seq1)<n_seq.unsqueeze(1)\n",
    "        \n",
    "        masked_loss=(loss*mask.unsqueeze(2)).sum()\n",
    "        masked_loss=masked_loss/mask.sum()\n",
    "    \n",
    "        masked_loss.backward()\n",
    "        running_loss+=masked_loss.item()\n",
    "        strat_ku_optimizer.step()\n",
    "    print(epoch,running_loss/len(strat_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W214 09:49:07.968910000 shape_type_inference.cpp:1999] Warning: The shape inference of prim::PackPadded type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W214 09:49:07.979180000 shape_type_inference.cpp:1999] Warning: The shape inference of prim::PackPadded type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W214 09:49:07.189027000 shape_type_inference.cpp:1999] Warning: The shape inference of prim::PadPacked type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W214 09:49:08.349283000 shape_type_inference.cpp:1999] Warning: The shape inference of prim::PackPadded type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W214 09:49:08.350388000 shape_type_inference.cpp:1999] Warning: The shape inference of prim::PackPadded type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W214 09:49:08.561301000 shape_type_inference.cpp:1999] Warning: The shape inference of prim::PadPacked type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W214 09:49:08.724100000 shape_type_inference.cpp:1999] Warning: The shape inference of prim::PackPadded type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W214 09:49:08.725097000 shape_type_inference.cpp:1999] Warning: The shape inference of prim::PackPadded type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W214 09:49:08.938520000 shape_type_inference.cpp:1999] Warning: The shape inference of prim::PadPacked type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W214 09:49:08.098713000 shape_type_inference.cpp:1999] Warning: The shape inference of prim::PackPadded type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W214 09:49:08.099974000 shape_type_inference.cpp:1999] Warning: The shape inference of prim::PackPadded type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W214 09:49:09.311591000 shape_type_inference.cpp:1999] Warning: The shape inference of prim::PadPacked type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W214 09:49:09.472420000 shape_type_inference.cpp:1999] Warning: The shape inference of prim::PackPadded type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W214 09:49:09.473691000 shape_type_inference.cpp:1999] Warning: The shape inference of prim::PackPadded type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W214 09:49:09.696954000 shape_type_inference.cpp:1999] Warning: The shape inference of prim::PadPacked type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_ens):\n",
    "    torch.onnx.export(model_list[i],(x[:1][:,:,:],n_seq[:1]),'strat_ku_ocean_feb4_2025_%2.2i.onnx'%i,export_params=True,opset_version=11,do_constant_folding=True,input_names=['input','n_seq'],output_names=['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_strat.eval()\n",
    "output=model_strat(x_data_strat_n[::2,:,:],n_seq_data_strat[::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_np=output.detach().numpy()\n",
    "import pickle\n",
    "pickle_out={\"output\":output_np,\"y_data\":y_data_strat_n[::2,:,:].detach().numpy(),\"x_data\":x_data_strat_n[::2,:,:].detach().numpy(),\"n_seq\":n_seq_data_strat[::2].detach().numpy(),\"z_meas\":z_meas2_strat[::2],\"bin_nodes\":bin_nodesL_strat[::2]}\n",
    "pickle.dump(pickle_out,open(\"output_strat_jan28_2025.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W129 11:23:13.674287000 shape_type_inference.cpp:1999] Warning: The shape inference of prim::PackPadded type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W129 11:23:13.675937000 shape_type_inference.cpp:1999] Warning: The shape inference of prim::PackPadded type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "/Users/mgrecu/miniforge3/lib/python3.12/site-packages/torch/onnx/symbolic_opset9.py:4279: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n",
      "[W129 11:23:13.885182000 shape_type_inference.cpp:1999] Warning: The shape inference of prim::PadPacked type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(model_ku_strat,(x[:1][:,:,a_feat],n_seq[:1]),'conv_and_strat_model_ku_jan28_2025.onnx',export_params=True,opset_version=11,do_constant_folding=True,input_names=['input','n_seq'],output_names=['output'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
