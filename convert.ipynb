{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 11])\n",
      "Exported graph: graph(%input : Float(1, 11, strides=[0, 1], requires_grad=0, device=cpu),\n",
      "      %0.weight : Float(12, 11, strides=[11, 1], requires_grad=0, device=cpu),\n",
      "      %0.bias : Float(12, strides=[1], requires_grad=0, device=cpu),\n",
      "      %2.weight : Float(12, 12, strides=[12, 1], requires_grad=0, device=cpu),\n",
      "      %2.bias : Float(12, strides=[1], requires_grad=0, device=cpu),\n",
      "      %4.weight : Float(4, 12, strides=[12, 1], requires_grad=0, device=cpu),\n",
      "      %4.bias : Float(4, strides=[1], requires_grad=0, device=cpu)):\n",
      "  %/0/Gemm_output_0 : Float(1, 12, strides=[12, 1], device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/0/Gemm\"](%input, %0.weight, %0.bias), scope: torch.nn.modules.container.Sequential::/torch.nn.modules.linear.Linear::0 # /Users/mgrecu/miniforge3/lib/python3.12/site-packages/torch/nn/modules/linear.py:125:15\n",
      "  %/1/Relu_output_0 : Float(1, 12, strides=[12, 1], device=cpu) = onnx::Relu[onnx_name=\"/1/Relu\"](%/0/Gemm_output_0), scope: torch.nn.modules.container.Sequential::/torch.nn.modules.activation.ReLU::1 # /Users/mgrecu/miniforge3/lib/python3.12/site-packages/torch/nn/functional.py:1702:17\n",
      "  %/2/Gemm_output_0 : Float(1, 12, strides=[12, 1], device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/2/Gemm\"](%/1/Relu_output_0, %2.weight, %2.bias), scope: torch.nn.modules.container.Sequential::/torch.nn.modules.linear.Linear::2 # /Users/mgrecu/miniforge3/lib/python3.12/site-packages/torch/nn/modules/linear.py:125:15\n",
      "  %/3/Relu_output_0 : Float(1, 12, strides=[12, 1], device=cpu) = onnx::Relu[onnx_name=\"/3/Relu\"](%/2/Gemm_output_0), scope: torch.nn.modules.container.Sequential::/torch.nn.modules.activation.ReLU::3 # /Users/mgrecu/miniforge3/lib/python3.12/site-packages/torch/nn/functional.py:1702:17\n",
      "  %enc_output : Float(1, 4, strides=[4, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/4/Gemm\"](%/3/Relu_output_0, %4.weight, %4.bias), scope: torch.nn.modules.container.Sequential::/torch.nn.modules.linear.Linear::4 # /Users/mgrecu/miniforge3/lib/python3.12/site-packages/torch/nn/modules/linear.py:125:15\n",
      "  return (%enc_output)\n",
      "\n",
      "Exported graph: graph(%enc_output : Float(1, 4, strides=[4, 1], requires_grad=0, device=cpu),\n",
      "      %0.weight : Float(12, 4, strides=[4, 1], requires_grad=0, device=cpu),\n",
      "      %0.bias : Float(12, strides=[1], requires_grad=0, device=cpu),\n",
      "      %2.weight : Float(12, 12, strides=[12, 1], requires_grad=0, device=cpu),\n",
      "      %2.bias : Float(12, strides=[1], requires_grad=0, device=cpu),\n",
      "      %4.weight : Float(11, 12, strides=[12, 1], requires_grad=0, device=cpu),\n",
      "      %4.bias : Float(11, strides=[1], requires_grad=0, device=cpu)):\n",
      "  %/0/Gemm_output_0 : Float(1, 12, strides=[12, 1], device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/0/Gemm\"](%enc_output, %0.weight, %0.bias), scope: torch.nn.modules.container.Sequential::/torch.nn.modules.linear.Linear::0 # /Users/mgrecu/miniforge3/lib/python3.12/site-packages/torch/nn/modules/linear.py:125:15\n",
      "  %/1/Relu_output_0 : Float(1, 12, strides=[12, 1], device=cpu) = onnx::Relu[onnx_name=\"/1/Relu\"](%/0/Gemm_output_0), scope: torch.nn.modules.container.Sequential::/torch.nn.modules.activation.ReLU::1 # /Users/mgrecu/miniforge3/lib/python3.12/site-packages/torch/nn/functional.py:1702:17\n",
      "  %/2/Gemm_output_0 : Float(1, 12, strides=[12, 1], device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/2/Gemm\"](%/1/Relu_output_0, %2.weight, %2.bias), scope: torch.nn.modules.container.Sequential::/torch.nn.modules.linear.Linear::2 # /Users/mgrecu/miniforge3/lib/python3.12/site-packages/torch/nn/modules/linear.py:125:15\n",
      "  %/3/Relu_output_0 : Float(1, 12, strides=[12, 1], device=cpu) = onnx::Relu[onnx_name=\"/3/Relu\"](%/2/Gemm_output_0), scope: torch.nn.modules.container.Sequential::/torch.nn.modules.activation.ReLU::3 # /Users/mgrecu/miniforge3/lib/python3.12/site-packages/torch/nn/functional.py:1702:17\n",
      "  %dec_output : Float(1, 11, strides=[11, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/4/Gemm\"](%/3/Relu_output_0, %4.weight, %4.bias), scope: torch.nn.modules.container.Sequential::/torch.nn.modules.linear.Linear::4 # /Users/mgrecu/miniforge3/lib/python3.12/site-packages/torch/nn/modules/linear.py:125:15\n",
      "  return (%dec_output)\n",
      "\n",
      "Exported graph: graph(%input : Float(1, 11, strides=[0, 1], requires_grad=0, device=cpu),\n",
      "      %0.weight : Float(12, 11, strides=[11, 1], requires_grad=0, device=cpu),\n",
      "      %0.bias : Float(12, strides=[1], requires_grad=0, device=cpu),\n",
      "      %2.weight : Float(12, 12, strides=[12, 1], requires_grad=0, device=cpu),\n",
      "      %2.bias : Float(12, strides=[1], requires_grad=0, device=cpu),\n",
      "      %4.weight : Float(4, 12, strides=[12, 1], requires_grad=0, device=cpu),\n",
      "      %4.bias : Float(4, strides=[1], requires_grad=0, device=cpu)):\n",
      "  %/0/Gemm_output_0 : Float(1, 12, strides=[12, 1], device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/0/Gemm\"](%input, %0.weight, %0.bias), scope: torch.nn.modules.container.Sequential::/torch.nn.modules.linear.Linear::0 # /Users/mgrecu/miniforge3/lib/python3.12/site-packages/torch/nn/modules/linear.py:125:15\n",
      "  %/1/Relu_output_0 : Float(1, 12, strides=[12, 1], device=cpu) = onnx::Relu[onnx_name=\"/1/Relu\"](%/0/Gemm_output_0), scope: torch.nn.modules.container.Sequential::/torch.nn.modules.activation.ReLU::1 # /Users/mgrecu/miniforge3/lib/python3.12/site-packages/torch/nn/functional.py:1702:17\n",
      "  %/2/Gemm_output_0 : Float(1, 12, strides=[12, 1], device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/2/Gemm\"](%/1/Relu_output_0, %2.weight, %2.bias), scope: torch.nn.modules.container.Sequential::/torch.nn.modules.linear.Linear::2 # /Users/mgrecu/miniforge3/lib/python3.12/site-packages/torch/nn/modules/linear.py:125:15\n",
      "  %/3/Relu_output_0 : Float(1, 12, strides=[12, 1], device=cpu) = onnx::Relu[onnx_name=\"/3/Relu\"](%/2/Gemm_output_0), scope: torch.nn.modules.container.Sequential::/torch.nn.modules.activation.ReLU::3 # /Users/mgrecu/miniforge3/lib/python3.12/site-packages/torch/nn/functional.py:1702:17\n",
      "  %enc_output : Float(1, 4, strides=[4, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/4/Gemm\"](%/3/Relu_output_0, %4.weight, %4.bias), scope: torch.nn.modules.container.Sequential::/torch.nn.modules.linear.Linear::4 # /Users/mgrecu/miniforge3/lib/python3.12/site-packages/torch/nn/modules/linear.py:125:15\n",
      "  return (%enc_output)\n",
      "\n",
      "Exported graph: graph(%enc_output : Float(1, 4, strides=[4, 1], requires_grad=0, device=cpu),\n",
      "      %0.weight : Float(12, 4, strides=[4, 1], requires_grad=0, device=cpu),\n",
      "      %0.bias : Float(12, strides=[1], requires_grad=0, device=cpu),\n",
      "      %2.weight : Float(12, 12, strides=[12, 1], requires_grad=0, device=cpu),\n",
      "      %2.bias : Float(12, strides=[1], requires_grad=0, device=cpu),\n",
      "      %4.weight : Float(11, 12, strides=[12, 1], requires_grad=0, device=cpu),\n",
      "      %4.bias : Float(11, strides=[1], requires_grad=0, device=cpu)):\n",
      "  %/0/Gemm_output_0 : Float(1, 12, strides=[12, 1], device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/0/Gemm\"](%enc_output, %0.weight, %0.bias), scope: torch.nn.modules.container.Sequential::/torch.nn.modules.linear.Linear::0 # /Users/mgrecu/miniforge3/lib/python3.12/site-packages/torch/nn/modules/linear.py:125:15\n",
      "  %/1/Relu_output_0 : Float(1, 12, strides=[12, 1], device=cpu) = onnx::Relu[onnx_name=\"/1/Relu\"](%/0/Gemm_output_0), scope: torch.nn.modules.container.Sequential::/torch.nn.modules.activation.ReLU::1 # /Users/mgrecu/miniforge3/lib/python3.12/site-packages/torch/nn/functional.py:1702:17\n",
      "  %/2/Gemm_output_0 : Float(1, 12, strides=[12, 1], device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/2/Gemm\"](%/1/Relu_output_0, %2.weight, %2.bias), scope: torch.nn.modules.container.Sequential::/torch.nn.modules.linear.Linear::2 # /Users/mgrecu/miniforge3/lib/python3.12/site-packages/torch/nn/modules/linear.py:125:15\n",
      "  %/3/Relu_output_0 : Float(1, 12, strides=[12, 1], device=cpu) = onnx::Relu[onnx_name=\"/3/Relu\"](%/2/Gemm_output_0), scope: torch.nn.modules.container.Sequential::/torch.nn.modules.activation.ReLU::3 # /Users/mgrecu/miniforge3/lib/python3.12/site-packages/torch/nn/functional.py:1702:17\n",
      "  %dec_output : Float(1, 11, strides=[11, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/4/Gemm\"](%/3/Relu_output_0, %4.weight, %4.bias), scope: torch.nn.modules.container.Sequential::/torch.nn.modules.linear.Linear::4 # /Users/mgrecu/miniforge3/lib/python3.12/site-packages/torch/nn/modules/linear.py:125:15\n",
      "  return (%dec_output)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mgrecu/miniforge3/lib/python3.12/site-packages/torch/onnx/utils.py:780: UserWarning: no signature found for builtin <built-in method __call__ of PyCapsule object at 0x10ea41ad0>, skipping _decide_input_format\n",
      "  warnings.warn(f\"{e}, skipping _decide_input_format\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dense_autoencoder = torch.jit.load('dense_autoencoder_land.pt')\n",
    "#print(dense_autoencoder)\n",
    "import pickle\n",
    "scaler_qv=pickle.load(open('scaler_ocean_qv.pkl','rb'))\n",
    "import numpy as np\n",
    "test_tensor= torch.tensor(scaler_qv['mean'][np.newaxis,:],dtype=torch.float32)\n",
    "print(test_tensor.shape)\n",
    "y_pred = dense_autoencoder(test_tensor)\n",
    "y_enc_pred=dense_autoencoder.encoder(test_tensor)\n",
    "torch.onnx.export(dense_autoencoder.encoder, test_tensor, \"GMI_ONNX_Models/dense_encoder_land.onnx\", verbose=True, input_names = ['input'], output_names = ['enc_output'])\n",
    "torch.onnx.export(dense_autoencoder.decoder, y_enc_pred.detach(), \"GMI_ONNX_Models/dense_decoder_land.onnx\", verbose=True, input_names = ['enc_output'], output_names = ['dec_output'])\n",
    "\n",
    "dense_autoencoder_ocean = torch.jit.load('dense_autoencoder_ocean.pt')\n",
    "torch.onnx.export(dense_autoencoder_ocean.encoder, test_tensor, \"GMI_ONNX_Models/dense_encoder_ocean.onnx\", verbose=True, input_names = ['input'], output_names = ['enc_output'])\n",
    "torch.onnx.export(dense_autoencoder_ocean.decoder, y_enc_pred.detach(), \"GMI_ONNX_Models/dense_decoder_ocean.onnx\", verbose=True, input_names = ['enc_output'], output_names = ['dec_output'])\n",
    "\n",
    "# convert save scaler qv to a npz file\n",
    "np.savez('scaler_ocean_qv.npz', mean=scaler_qv['mean'], std=scaler_qv['std'])\n",
    "scaler_qv_land=pickle.load(open('scaler_land_qv.pkl','rb'))\n",
    "np.savez('scaler_land_qv.npz', mean=scaler_qv_land['mean'], std=scaler_qv_land['std'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(scaler_qv.keys())\n",
    "import numpy as np\n",
    "vars_string=['tc','sfc_type','sk_temp','oe_wvp','near_sfc_precip','xenc','xenc_prec','xenv_enc']\n",
    "#dict_scaler={\"vars\":vars_string,\"norm_param\":norm_param_L}\n",
    "import pickle\n",
    "#with open('norm_param_ocean.pkl','wb') as f:\n",
    "#    pickle.dump(dict_scaler,f)\n",
    "dict_scaler=pickle.load(open('norm_param_150_ocean.pkl','rb'))\n",
    "np.savez('GMI_ONNX_Models/norm_param_150_ocean.npz',tc=dict_scaler['norm_param'][0],sfc_type=dict_scaler['norm_param'][1],sk_temp=dict_scaler['norm_param'][2],oe_wvp=dict_scaler['norm_param'][3],near_sfc_precip=dict_scaler['norm_param'][4],xenc=dict_scaler['norm_param'][5],xenc_prec=dict_scaler['norm_param'][6],xenv_enc=dict_scaler['norm_param'][7])\n",
    "dict_scaler_land=pickle.load(open('norm_param_150_land.pkl','rb'))\n",
    "np.savez('GMI_ONNX_Models/norm_param_150_land.npz',tc=dict_scaler_land['norm_param'][0],sfc_type=dict_scaler_land['norm_param'][1],sk_temp=dict_scaler_land['norm_param'][2],oe_wvp=dict_scaler_land['norm_param'][3],near_sfc_precip=dict_scaler_land['norm_param'][4],xenc=dict_scaler_land['norm_param'][5],xenc_prec=dict_scaler_land['norm_param'][6],xenv_enc=dict_scaler_land['norm_param'][7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.savez('GMI_ONNX_Models/var_string.npz',vars=vars_string)\n",
    "np.savetxt('GMI_ONNX_Models/var_string.txt',vars_string,fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inputs': [{'name': 'input', 'shape': [1, 11], 'dtype': 1}], 'outputs': [{'name': 'enc_output', 'shape': [1, 4], 'dtype': 1}]}\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "def get_model_io_details(model_path):\n",
    "    model = onnx.load(model_path)\n",
    "    graph = model.graph\n",
    "\n",
    "    def get_details(value_info):\n",
    "        name = value_info.name\n",
    "        tensor_type = value_info.type.tensor_type\n",
    "        shape = [dim.dim_value if dim.HasField(\"dim_value\") else \"?\" for dim in tensor_type.shape.dim]\n",
    "        dtype = tensor_type.elem_type\n",
    "        return {\"name\": name, \"shape\": shape, \"dtype\": dtype}\n",
    "\n",
    "    inputs = [get_details(input) for input in graph.input]\n",
    "    outputs = [get_details(output) for output in graph.output]\n",
    "\n",
    "    return {\"inputs\": inputs, \"outputs\": outputs}\n",
    "\n",
    "# Example usage:\n",
    "#model_details = get_model_io_details(\"model.onnx\")\n",
    "#print(model_details)\n",
    "\n",
    "model_details = get_model_io_details(\"GMI_ONNX_Models/dense_encoder_land.onnx\")\n",
    "print(model_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GMI_ONNX_Models/dense_autoencoder_land.onnx', 'GMI_ONNX_Models/dense_autoencoder_ocean.onnx', 'GMI_ONNX_Models/dense_decoder_land.onnx', 'GMI_ONNX_Models/dense_decoder_ocean.onnx', 'GMI_ONNX_Models/dense_encoder_land.onnx', 'GMI_ONNX_Models/dense_encoder_ocean.onnx', 'GMI_ONNX_Models/land_model.onnx', 'GMI_ONNX_Models/ocean_model.onnx']\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "# Mapping ONNX TensorProto values to readable dtype names\n",
    "ONNX_DTYPE_MAP = {\n",
    "    1: \"FLOAT32\",\n",
    "    2: \"UINT8\",\n",
    "    3: \"INT8\",\n",
    "    4: \"UINT16\",\n",
    "    5: \"INT16\",\n",
    "    6: \"INT32\",\n",
    "    7: \"INT64\",\n",
    "    8: \"STRING\",\n",
    "    9: \"BOOL\",\n",
    "    10: \"FLOAT16\",\n",
    "    11: \"FLOAT64\",\n",
    "    12: \"UINT32\",\n",
    "    13: \"UINT64\",\n",
    "    14: \"COMPLEX64\",\n",
    "    15: \"COMPLEX128\",\n",
    "    16: \"BFLOAT16\",\n",
    "}\n",
    "\n",
    "def get_model_io_details(model_path):\n",
    "    model = onnx.load(model_path)\n",
    "    graph = model.graph\n",
    "\n",
    "    def get_details(value_info):\n",
    "        name = value_info.name\n",
    "        tensor_type = value_info.type.tensor_type\n",
    "        shape = [dim.dim_value if dim.HasField(\"dim_value\") else \"?\" for dim in tensor_type.shape.dim]\n",
    "        dtype = ONNX_DTYPE_MAP.get(tensor_type.elem_type, f\"Unknown({tensor_type.elem_type})\")\n",
    "        return {\"name\": name, \"shape\": shape, \"dtype\": dtype}\n",
    "\n",
    "    inputs = [get_details(input) for input in graph.input]\n",
    "    outputs = [get_details(output) for output in graph.output]\n",
    "\n",
    "    return {\"inputs\": inputs, \"outputs\": outputs}\n",
    "\n",
    "# Example usage:\n",
    "#model_details = get_model_io_details(\"model.onnx\")\n",
    "#print(model_details)\n",
    "import glob\n",
    "model_paths=glob.glob(\"GMI_ONNX_Models/*.onnx\")\n",
    "\n",
    "model_paths=sorted(model_paths)\n",
    "print(model_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_details_list=[]\n",
    "for model_path in model_paths:\n",
    "    model_details = get_model_io_details(model_path)\n",
    "    print(model_path,model_details)\n",
    "    model_details_list.append(model_details)\n",
    "#model_details = get_model_io_details(\"GMI_ONNX_Models/dense_encoder_land.onnx\")\n",
    "#print(model_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 GMI_ONNX_Models/dense_autoencoder_land.onnx\n",
      "{'inputs': [{'name': 'input', 'shape': [1, 11], 'dtype': 'FLOAT32'}], 'outputs': [{'name': 'output', 'shape': [1, 11], 'dtype': 'FLOAT32'}]}\n",
      "1 GMI_ONNX_Models/dense_autoencoder_ocean.onnx\n",
      "{'inputs': [{'name': 'input', 'shape': [1, 11], 'dtype': 'FLOAT32'}], 'outputs': [{'name': 'output', 'shape': [1, 11], 'dtype': 'FLOAT32'}]}\n",
      "2 GMI_ONNX_Models/dense_decoder_land.onnx\n",
      "{'inputs': [{'name': 'enc_output', 'shape': [1, 4], 'dtype': 'FLOAT32'}], 'outputs': [{'name': 'dec_output', 'shape': [1, 11], 'dtype': 'FLOAT32'}]}\n",
      "3 GMI_ONNX_Models/dense_decoder_ocean.onnx\n",
      "{'inputs': [{'name': 'enc_output', 'shape': [1, 4], 'dtype': 'FLOAT32'}], 'outputs': [{'name': 'dec_output', 'shape': [1, 11], 'dtype': 'FLOAT32'}]}\n",
      "4 GMI_ONNX_Models/dense_encoder_land.onnx\n",
      "{'inputs': [{'name': 'input', 'shape': [1, 11], 'dtype': 'FLOAT32'}], 'outputs': [{'name': 'enc_output', 'shape': [1, 4], 'dtype': 'FLOAT32'}]}\n",
      "5 GMI_ONNX_Models/dense_encoder_ocean.onnx\n",
      "{'inputs': [{'name': 'input', 'shape': [1, 11], 'dtype': 'FLOAT32'}], 'outputs': [{'name': 'enc_output', 'shape': [1, 4], 'dtype': 'FLOAT32'}]}\n",
      "6 GMI_ONNX_Models/land_model.onnx\n",
      "{'inputs': [{'name': 'input', 'shape': [1, 19, 150, 49], 'dtype': 'FLOAT32'}], 'outputs': [{'name': 'output_rec', 'shape': [1, 13, 150, 49], 'dtype': 'FLOAT32'}, {'name': 'output_pred', 'shape': [1, 12, 150, 49], 'dtype': 'FLOAT32'}]}\n",
      "7 GMI_ONNX_Models/ocean_model.onnx\n",
      "{'inputs': [{'name': 'input', 'shape': [1, 19, 150, 49], 'dtype': 'FLOAT32'}], 'outputs': [{'name': 'output_rec', 'shape': [1, 13, 150, 49], 'dtype': 'FLOAT32'}, {'name': 'output_pred', 'shape': [1, 12, 150, 49], 'dtype': 'FLOAT32'}]}\n"
     ]
    }
   ],
   "source": [
    "for i,model_details in enumerate(model_details_list):\n",
    "    print(i,model_paths[i])\n",
    "    print(model_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strcpy(models[0].inputs[0].name,\"input\")\n",
      "models[0].inputs[0].shape_len=2\n",
      "models[0].inputs[0].shape[0]=1\n",
      "models[0].inputs[0].shape[1]=11\n",
      "models[0].inputs[0].dtype=1\n",
      "models[0].input_data[0]=(float *) malloc(sizeof(float)*11)\n",
      "strcpy(models[0].outputs[0].name,\"output\")\n",
      "models[0].outputs[0].shape_len=2\n",
      "models[0].outputs[0].shape[0]=1\n",
      "models[0].outputs[0].shape[1]=11\n",
      "models[0].outputs[0].dtype=1\n",
      "models[0].output_data[0]=(float *) malloc(sizeof(float)*11)\n",
      "strcpy(models[1].inputs[0].name,\"input\")\n",
      "models[1].inputs[0].shape_len=2\n",
      "models[1].inputs[0].shape[0]=1\n",
      "models[1].inputs[0].shape[1]=11\n",
      "models[1].inputs[0].dtype=1\n",
      "models[1].input_data[0]=(float *) malloc(sizeof(float)*11)\n",
      "strcpy(models[1].outputs[0].name,\"output\")\n",
      "models[1].outputs[0].shape_len=2\n",
      "models[1].outputs[0].shape[0]=1\n",
      "models[1].outputs[0].shape[1]=11\n",
      "models[1].outputs[0].dtype=1\n",
      "models[1].output_data[0]=(float *) malloc(sizeof(float)*11)\n",
      "strcpy(models[2].inputs[0].name,\"enc_output\")\n",
      "models[2].inputs[0].shape_len=2\n",
      "models[2].inputs[0].shape[0]=1\n",
      "models[2].inputs[0].shape[1]=4\n",
      "models[2].inputs[0].dtype=1\n",
      "models[2].input_data[0]=(float *) malloc(sizeof(float)*4)\n",
      "strcpy(models[2].outputs[0].name,\"dec_output\")\n",
      "models[2].outputs[0].shape_len=2\n",
      "models[2].outputs[0].shape[0]=1\n",
      "models[2].outputs[0].shape[1]=11\n",
      "models[2].outputs[0].dtype=1\n",
      "models[2].output_data[0]=(float *) malloc(sizeof(float)*11)\n",
      "strcpy(models[3].inputs[0].name,\"enc_output\")\n",
      "models[3].inputs[0].shape_len=2\n",
      "models[3].inputs[0].shape[0]=1\n",
      "models[3].inputs[0].shape[1]=4\n",
      "models[3].inputs[0].dtype=1\n",
      "models[3].input_data[0]=(float *) malloc(sizeof(float)*4)\n",
      "strcpy(models[3].outputs[0].name,\"dec_output\")\n",
      "models[3].outputs[0].shape_len=2\n",
      "models[3].outputs[0].shape[0]=1\n",
      "models[3].outputs[0].shape[1]=11\n",
      "models[3].outputs[0].dtype=1\n",
      "models[3].output_data[0]=(float *) malloc(sizeof(float)*11)\n",
      "strcpy(models[4].inputs[0].name,\"input\")\n",
      "models[4].inputs[0].shape_len=2\n",
      "models[4].inputs[0].shape[0]=1\n",
      "models[4].inputs[0].shape[1]=11\n",
      "models[4].inputs[0].dtype=1\n",
      "models[4].input_data[0]=(float *) malloc(sizeof(float)*11)\n",
      "strcpy(models[4].outputs[0].name,\"enc_output\")\n",
      "models[4].outputs[0].shape_len=2\n",
      "models[4].outputs[0].shape[0]=1\n",
      "models[4].outputs[0].shape[1]=4\n",
      "models[4].outputs[0].dtype=1\n",
      "models[4].output_data[0]=(float *) malloc(sizeof(float)*4)\n",
      "strcpy(models[5].inputs[0].name,\"input\")\n",
      "models[5].inputs[0].shape_len=2\n",
      "models[5].inputs[0].shape[0]=1\n",
      "models[5].inputs[0].shape[1]=11\n",
      "models[5].inputs[0].dtype=1\n",
      "models[5].input_data[0]=(float *) malloc(sizeof(float)*11)\n",
      "strcpy(models[5].outputs[0].name,\"enc_output\")\n",
      "models[5].outputs[0].shape_len=2\n",
      "models[5].outputs[0].shape[0]=1\n",
      "models[5].outputs[0].shape[1]=4\n",
      "models[5].outputs[0].dtype=1\n",
      "models[5].output_data[0]=(float *) malloc(sizeof(float)*4)\n",
      "strcpy(models[6].inputs[0].name,\"input\")\n",
      "models[6].inputs[0].shape_len=4\n",
      "models[6].inputs[0].shape[0]=1\n",
      "models[6].inputs[0].shape[1]=19\n",
      "models[6].inputs[0].shape[2]=150\n",
      "models[6].inputs[0].shape[3]=49\n",
      "models[6].inputs[0].dtype=1\n",
      "models[6].input_data[0]=(float *) malloc(sizeof(float)*139650)\n",
      "strcpy(models[6].outputs[0].name,\"output_rec\")\n",
      "models[6].outputs[0].shape_len=4\n",
      "models[6].outputs[0].shape[0]=1\n",
      "models[6].outputs[0].shape[1]=13\n",
      "models[6].outputs[0].shape[2]=150\n",
      "models[6].outputs[0].shape[3]=49\n",
      "models[6].outputs[0].dtype=1\n",
      "models[6].output_data[0]=(float *) malloc(sizeof(float)*95550)\n",
      "strcpy(models[6].outputs[1].name,\"output_pred\")\n",
      "models[6].outputs[1].shape_len=4\n",
      "models[6].outputs[1].shape[0]=1\n",
      "models[6].outputs[1].shape[1]=12\n",
      "models[6].outputs[1].shape[2]=150\n",
      "models[6].outputs[1].shape[3]=49\n",
      "models[6].outputs[1].dtype=1\n",
      "models[6].output_data[1]=(float *) malloc(sizeof(float)*88200)\n",
      "strcpy(models[7].inputs[0].name,\"input\")\n",
      "models[7].inputs[0].shape_len=4\n",
      "models[7].inputs[0].shape[0]=1\n",
      "models[7].inputs[0].shape[1]=19\n",
      "models[7].inputs[0].shape[2]=150\n",
      "models[7].inputs[0].shape[3]=49\n",
      "models[7].inputs[0].dtype=1\n",
      "models[7].input_data[0]=(float *) malloc(sizeof(float)*139650)\n",
      "strcpy(models[7].outputs[0].name,\"output_rec\")\n",
      "models[7].outputs[0].shape_len=4\n",
      "models[7].outputs[0].shape[0]=1\n",
      "models[7].outputs[0].shape[1]=13\n",
      "models[7].outputs[0].shape[2]=150\n",
      "models[7].outputs[0].shape[3]=49\n",
      "models[7].outputs[0].dtype=1\n",
      "models[7].output_data[0]=(float *) malloc(sizeof(float)*95550)\n",
      "strcpy(models[7].outputs[1].name,\"output_pred\")\n",
      "models[7].outputs[1].shape_len=4\n",
      "models[7].outputs[1].shape[0]=1\n",
      "models[7].outputs[1].shape[1]=12\n",
      "models[7].outputs[1].shape[2]=150\n",
      "models[7].outputs[1].shape[3]=49\n",
      "models[7].outputs[1].dtype=1\n",
      "models[7].output_data[1]=(float *) malloc(sizeof(float)*88200)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "f_mod=open(\"model_details.h\",\"w\")\n",
    "f_mod.write(\"void models_details_c_(){\\n\")    \n",
    "for i,model_details in enumerate(model_details_list):\n",
    "    stat_path=\"strcpy(models[%i].model_path,\\\"%s\\\");\\n\"%(i,model_paths[i])\n",
    "    f_mod.write(stat_path)\n",
    "    print_path=\"printf(\\\"model[%i] =%%s \\\\n\\\",models[%i].model_path);\\n\"%(i,i)\n",
    "    f_mod.write(print_path)\n",
    "    for i_in,inputs in enumerate(model_details['inputs']):\n",
    "        stat_shape=\"models[%i].inputs[%i].shape_len=%i\"%(i,i_in,len(inputs['shape']))\n",
    "        stat_name=\"strcpy(models[%i].inputs[%i].name,\\\"%s\\\")\"%(i,i_in,inputs['name'])\n",
    "        stat_nelem=\"models[%i].inputs[%i].n_elem=%i\"%(i,i_in,np.prod(inputs['shape']))\n",
    "        print(stat_name)\n",
    "        f_mod.write(stat_name+';\\n')\n",
    "        print(stat_shape)\n",
    "        f_mod.write(stat_shape+';\\n')\n",
    "        f_mod.write(stat_nelem+';\\n')\n",
    "        total_size=1\n",
    "        for k in range(len(inputs['shape'])):\n",
    "            stat_shape_def=\"models[%i].inputs[%i].shape[%i]=%i\"%(i,i_in,k,inputs['shape'][k])\n",
    "            total_size*=inputs['shape'][k]\n",
    "            print(stat_shape_def)\n",
    "            f_mod.write(stat_shape_def+';\\n')\n",
    "        if inputs['dtype']=='FLOAT32':\n",
    "            stat_type=\"models[%i].inputs[%i].dtype=1\"%(i,i_in)\n",
    "        print(stat_type)\n",
    "        f_mod.write(stat_type+';\\n')\n",
    "        #allocate input tensor\n",
    "        stat_alloc=\"models[%i].input_data[%i]=(float *) malloc(sizeof(float)*%i)\"%(i,i_in,total_size)\n",
    "        print(stat_alloc)\n",
    "        f_mod.write(stat_alloc+';\\n')\n",
    "    stat_num_inputs=\"models[%i].num_inputs=%i\"%(i,len(model_details['inputs']))\n",
    "    f_mod.write(stat_num_inputs+';\\n')\n",
    "    stat_num_outputs=\"models[%i].num_outputs=%i\"%(i,len(model_details['outputs']))\n",
    "    f_mod.write(stat_num_outputs+';\\n')\n",
    "    for i_out,outputs in enumerate(model_details['outputs']):\n",
    "        stat_shape=\"models[%i].outputs[%i].shape_len=%i\"%(i,i_out,len(outputs['shape']))\n",
    "        stat_name=\"strcpy(models[%i].outputs[%i].name,\\\"%s\\\")\"%(i,i_out,outputs['name'])\n",
    "        stat_nelem=\"models[%i].outputs[%i].n_elem=%i\"%(i,i_out,np.prod(outputs['shape']))\n",
    "        print(stat_name)\n",
    "        f_mod.write(stat_name+';\\n')\n",
    "        print(stat_shape)\n",
    "        f_mod.write(stat_shape+';\\n')\n",
    "        f_mod.write(stat_nelem+';\\n')\n",
    "        total_size=1\n",
    "        for k in range(len(outputs['shape'])):\n",
    "            stat_shape_def=\"models[%i].outputs[%i].shape[%i]=%i\"%(i,i_out,k,outputs['shape'][k])\n",
    "            total_size*=outputs['shape'][k]\n",
    "            print(stat_shape_def)\n",
    "            f_mod.write(stat_shape_def+';\\n')\n",
    "        if outputs['dtype']=='FLOAT32':\n",
    "            stat_type=\"models[%i].outputs[%i].dtype=1\"%(i,i_out)\n",
    "        print(stat_type)\n",
    "        f_mod.write(stat_type+';\\n')\n",
    "        stat_alloc=\"models[%i].output_data[%i]=(float *) malloc(sizeof(float)*%i)\"%(i,i_out,total_size)\n",
    "        print(stat_alloc)\n",
    "        f_mod.write(stat_alloc+';\\n')\n",
    "\n",
    "f_mod.write(\"}\\n\")\n",
    "f_mod.close()\n",
    "        #print(inputs['name'],inputs['shape'],inputs['dtype'])\n",
    "#for outputs in model_details['outputs']:\n",
    "#    print(outputs['name'],outputs['shape'],outputs['dtype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(inputs['shape'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could you extend the code below to be more general and work with multiple models defined using information of the type \n",
    "GMI_ONNX_Models/land_model.onnx {'inputs': [{'name': 'input', 'shape': [1, 19, 128, 48], 'dtype': 'FLOAT32'}], 'outputs': [{'name': 'output_rec', 'shape': [1, 13, 128, 48], 'dtype': 'FLOAT32'}, {'name': 'output_pred', 'shape': [1, 12, 128, 48], 'dtype': 'FLOAT32'}]}?\n",
    "\n",
    "The input and ouput information could be stored in simple C structures emulating python general lists.\n",
    "\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <onnxruntime_c_api.h>\n",
    "\n",
    "// Global variables for the ONNX Runtime environment, session, and input/output tensors\n",
    "OrtEnv* env;\n",
    "OrtSession* session;\n",
    "OrtSessionOptions* session_options;\n",
    "//OrtAllocator* allocator;\n",
    "OrtValue* input_tensor;\n",
    "OrtValue* output_tensor;\n",
    "OrtMemoryInfo* memory_info;\n",
    "OrtStatus *status;\n",
    "// Initialization function\n",
    "OrtApi *g_api[4];\n",
    "\n",
    "\n",
    "void init_onnx_runtime_(void) {\n",
    "    // Initialize the ONNX Runtime environment\n",
    "    const OrtApi *api = OrtGetApiBase()->GetApi(ORT_API_VERSION);\n",
    "    //\"conv_and_strat_model_kuka_jan24_2025.onnx\"\n",
    "    for(int im=0;im<4;im++)\n",
    "      {\n",
    "\tchar model_path[150];\n",
    "\tif(im==0)\n",
    "\t  strcpy(model_path,\"onnx_models/conv_and_strat_model_ku_feb4_2025_01.onnx\");\n",
    "\tif(im==1)\n",
    "\t  strcpy(model_path,\"onnx_models/conv_ku_feb4_2025_01.onnx\");\n",
    "\tif(im==2)\n",
    "\t  strcpy(model_path,\"onnx_models/strat_ku_ocean_feb4_2025_01.onnx\");\n",
    "\tif(im==3)\n",
    "\t  strcpy(model_path,\"onnx_models/conv_ku_ocean_feb4_2025_01.onnx\");\n",
    "\tg_api[im] = (OrtApi *)api;\n",
    "\tstatus = g_api[im]->CreateEnv(ORT_LOGGING_LEVEL_WARNING, \"test\", &env);\n",
    "\tstatus = g_api[im]->CreateSessionOptions(&session_options);\n",
    "\tstatus = g_api[im]->SetIntraOpNumThreads(session_options, 1);\n",
    "\tstatus = g_api[im]->CreateSession(env, model_path, session_options, &session);\n",
    "\tif (status != NULL) {\n",
    "\t  const char* msg = g_api[im]->GetErrorMessage(status);\n",
    "\t  fprintf(stderr, \"Failed to run ONNX model: %s\\n\", msg);\n",
    "\t  g_api[im]->ReleaseStatus(status);\n",
    "\t}\n",
    "\tstatus = g_api[im]->CreateCpuMemoryInfo(OrtArenaAllocator, OrtMemTypeDefault, &memory_info);\n",
    "      }\n",
    "\n",
    "}\n",
    "\n",
    "void call_onnx_(float *input_data, int *lengths_data, float *output_data, int *batch_size, int *seq_len, int *input_size, int *output_size, int *im) {\n",
    "    //'input': input_data, 'n_seq'\n",
    "    const char* input_names[] = {\"input\", \"n_seq\"};\n",
    "    const char* output_names[] = {\"output\"};\n",
    "\n",
    "    int64_t input_shape[3] = {(int64_t)(*batch_size), (int64_t)(*seq_len), (int64_t)(*input_size)};\n",
    "    size_t input_tensor_size = (*batch_size) * (*seq_len) * (*input_size);\n",
    "    OrtValue* input_tensor = NULL;\n",
    "\n",
    "    status=g_api[*im]->CreateTensorWithDataAsOrtValue(\n",
    "        memory_info, input_data, input_tensor_size * sizeof(float),\n",
    "        input_shape, 3, ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT, &input_tensor);\n",
    "    //printf(\"batch_size: %d\\n\", *batch_size);\n",
    "    //printf(\"seq_len: %d\\n\", *seq_len);\n",
    "    //printf(\"input_size: %d\\n\", *input_size);\n",
    "    //printf(\"output_size: %d\\n\", *output_size);\n",
    "    // Prepare seq_lengths tensor\n",
    "    int64_t lengths_shape[1] = {(int32_t)(*batch_size)};\n",
    "    OrtValue* lengths_tensor = NULL;\n",
    "    int32_t *lengths_data_t;\n",
    "    lengths_data_t = (int32_t *)malloc((*batch_size) * sizeof(int32_t));\n",
    "    for (int i = 0; i < *batch_size; i++) {\n",
    "        lengths_data_t[i] = (int32_t)lengths_data[i];\n",
    "    }\n",
    "    status=g_api[*im]->CreateTensorWithDataAsOrtValue(\n",
    "        memory_info, lengths_data_t, (*batch_size) * sizeof(int32_t),\n",
    "        lengths_shape, 1, ONNX_TENSOR_ELEMENT_DATA_TYPE_INT32, &lengths_tensor);\n",
    "    \n",
    "    if (status != NULL) {\n",
    "        const char* msg = g_api[*im]->GetErrorMessage(status);\n",
    "        fprintf(stderr, \"Failed to run ONNX model: %s\\n\", msg);\n",
    "        fprintf(stderr, \"batch_size: %d\\n\", *batch_size);\n",
    "        g_api[*im]->ReleaseStatus(status);\n",
    "    }\n",
    "    // Create input names and tensors\n",
    "    const OrtValue* input_tensors[] = {input_tensor, lengths_tensor};\n",
    "\n",
    "    // Prepare output tensor\n",
    "    int64_t output_shape[3] = {(int64_t)(*batch_size), (int64_t)(*seq_len), (int64_t)(*output_size)};\n",
    "    size_t output_tensor_size = (*batch_size) * (*seq_len) * (*output_size);\n",
    "    OrtValue* output_tensor = NULL;\n",
    "\n",
    "    status=g_api[*im]->CreateTensorWithDataAsOrtValue(\n",
    "        memory_info, output_data, output_tensor_size * sizeof(float),\n",
    "        output_shape, 3, ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT, &output_tensor);\n",
    "\n",
    "    \n",
    "    // Run the model\n",
    "    status = g_api[*im]->Run(\n",
    "        session, NULL, input_names, input_tensors, 2,\n",
    "        output_names, 1, &output_tensor);\n",
    "\n",
    "    if (status != NULL) {\n",
    "        const char* msg = g_api[*im]->GetErrorMessage(status);\n",
    "        fprintf(stderr, \"Failed to run ONNX model: %s\\n\", msg);\n",
    "        g_api[*im]->ReleaseStatus(status);\n",
    "    }\n",
    "\n",
    "    // Release resources\n",
    "    g_api[*im]->ReleaseValue(input_tensor);\n",
    "    g_api[*im]->ReleaseValue(lengths_tensor);\n",
    "    g_api[*im]->ReleaseValue(output_tensor);\n",
    "    free(lengths_data_t);\n",
    "}\n",
    "GMI_ONNX_Models/land_model.onnx {'inputs': [{'name': 'input', 'shape': [1, 19, 128, 48], 'dtype': 'FLOAT32'}], 'outputs': [{'name': 'output_rec', 'shape': [1, 13, 128, 48], 'dtype': 'FLOAT32'}, {'name': 'output_pred', 'shape': [1, 12, 128, 48], 'dtype': 'FLOAT32'}]}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
